{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto I. Desarrollar un Modelo de Lenguaje desde Cero usando PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente proyecto invita a los interesados a embarcarse en la construcción de modelos de lenguaje desde cero, empleando técnicas accesibles para generar texto de manera creativa. Se utilizará un conjunto de datos de nombres para demostrar cómo distintos modelos pueden producir resultados realistas y atractivos. El proyecto inicia con modelos básicos de bigramas y trigramas para comprender sus capacidades y avanza hacia una Red Neuronal Recurrente (RNN) más sofisticada, ampliando así las posibilidades creativas.\n",
    "\n",
    "**Fase I. Construcción de un Modelo Básico de Bigrama**\n",
    "\n",
    "En esta fase, se desarrollará un modelo básico de bigramas para generar y optimizar nombres utilizando trigramas, permitiendo apreciar las ventajas de modelos de n-gramas más complejos.\n",
    "\n",
    "**Fase II. Implementación de Redes Neuronales**\n",
    "\n",
    "En esta fase, se implementarán redes neuronales para perfeccionar el modelo. Se diseñará una RNN personalizada que capture patrones profundos en los nombres. Después de entrenarla, se utilizará esta RNN para generar nombres que demuestren la potencia de las redes neuronales en la creación de textos naturales y coherentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase I. Fundamentos de la Modelización del Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "En este innovador proyecto, los interesados diseñarán modelos de bigramas y trigramas, así como un avanzado modelo generador de nombres basado en redes neuronales. Se aprovecharán potentes bibliotecas de Python en cada etapa del desarrollo, desde el procesamiento de datos hasta la construcción y optimización meticulosa de la red neuronal.\n",
    "\n",
    "El repositorio de [GitHub](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/tree/main/Data_Science_Projects/Proyecto_I) contiene los archivos esenciales para completar el proyecto:\n",
    "\n",
    "- [/Proyecto_I/nombres.txt](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/nombres.txt): Archivo que contiene los nombres que se utilizarán para entrenar el modelo.\n",
    "- [/Proyecto_I/proyecto.ipynb](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/proyecto.ipynb): Cuaderno de Python donde se implementará el modelo. Cada sección del proyecto está asociada a una o varias celdas en el cuaderno, identificadas por su encabezado correspondiente.\n",
    "\n",
    "Al finalizar este proyecto, los participantes adquirirán un profundo conocimiento sobre la creación de modelos de lenguaje basados en bigramas y trigramas para generar texto a partir de secuencias de caracteres. Además, serán capaces de construir una Red Neuronal Recurrente personalizada en PyTorch para generar nombres únicos y aprenderán a optimizar el rendimiento del modelo mediante técnicas avanzadas de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importar los Módulos Necesarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se configura el entorno y cuaderno de Jupyter proporcionado para construir el modelo de lenguaje desde cero. El cuaderno [/Proyecto_I/proyecto.ipynb](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/proyecto.ipynb) será el espacio de trabajo principal para implementar las secciones del proyecto.\n",
    "\n",
    "En principio, se importan los módulos de Python necesarios, incluyendo `torch`, `torch.nn`, `torch.optim`, `matplotlib.pyplot` y `random`. Dichos módulos se utilizan de manera extensiva a lo largo del proyecto para desarrollar y visualizar el modelo de lenguaje.\n",
    "\n",
    "- `numpy`: Biblioteca que se emplea para realizar cálculos matemáticos y manipulaciones en arreglos y matrices, entre otros.\n",
    "- `torch`: Biblioteca principal de PyTorch, utilizada para el cálculo de tensores y la construcción de redes neuronales.\n",
    "- `torch.nn`: Submódulo de PyTorch que proporciona clases y funciones para construir capas y arquitecturas de redes neuronales.\n",
    "- `torch.optim`: Submódulo de PyTorch que se utiliza para la optimización de algoritmos.\n",
    "- `matplotlib.pyplot`: Biblioteca de gráficos que se emplea para crear visualizaciones estáticas, interactivas y animadas en Python.\n",
    "- `random`:  Módulo que ofrece funciones para generar números aleatorios y realizar operaciones aleatorias.\n",
    "\n",
    "Ejecutar el siguiente código en el archivo [/Proyecto_I/proyecto.ipynb](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/proyecto.ipynb) para importar los módulos necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importar módulos necesarios\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota:** Utilizar `Shift + Enter` para ejecutar una sola celda en el cuaderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cargar y Preprocesar los Datos de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se carga un conjunto de datos de nombres desde un archivo de texto y se preprocesa convirtiéndolos a minúsculas y añadiendo marcadores de inicio y fin. Luego, se calcula la frecuencia de bigramas (secuencias de dos caracteres consecutivos) en el conjunto de datos, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se abre el archivo [/Proyecto_I/nombres.txt](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/nombres.txt) que contiene el conjunto de datos de nombres y se divide su contenido en una lista de nombres individuales.\n",
    "2. Se itera a través de cada nombre y se preprocesa convirtiéndolo a minúsculas y añadiendo marcadores de inicio y fin (`<`, `>`).\n",
    "\n",
    "Ejecutar el siguiente código para cargar los datos desde el archivo de texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "with open('nombres.txt', 'r') as archivo:\n",
    "    nombres = archivo.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el siguiente código para preprocesar los nombres convirtiéndolos a minúsculas y añadiendo marcadores de inicio y fin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocesamiento de nombres\n",
    "nombres = ['<' + nombre.lower() + '>' for nombre in nombres]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota:** El conjunto de datos para este proyecto es una lista de nombres ubicada en [/Proyecto_I/nombres.txt](https://github.com/Jeshua-Romero-Guadarrama/LinkedIn-Jeshua-Romero-Guadarrama/blob/main/Data_Science_Projects/Proyecto_I/nombres.txt) y obtenida del enlace asociado a la [Seguridad Social](https://www.ssa.gov/oact/babynames/limits.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Construir y Visualizar la Tabla de Bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se crea una tabla de búsqueda para analizar la ocurrencia de bigramas en el conjunto de datos de nombres. La tabla de búsqueda se construye y visualiza la frecuencia de ocurrencia de bigramas, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se inicializa la `tabla_busqueda` para contar las ocurrencias de bigramas. Tiene tamaño 28×28 e incluye 26 letras del alfabeto inglés y 2 caracteres especiales.\n",
    "2. Después de inicializar la tabla de búsqueda, se crea un codificador `char_a_int` para mapear cada carácter a un entero.\n",
    "3. Se itera a través de la lista de nombres para contar las ocurrencias de cada bigrama y se almacenan esos conteos en la tabla de búsqueda.\n",
    "4. Se grafica la tabla de búsqueda mediante un diagrama de dispersión para visualizar la frecuencia de ocurrencia de bigramas.\n",
    "\n",
    "Ejecutar el siguiente código para inicializar y ordenar el vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inicializar y ordenar el vocabulario\n",
    "vocabulario    = set(''.join(nombres))\n",
    "vocabulario    = ''.join(sorted(vocabulario))\n",
    "tabla_busqueda = torch.zeros((len(vocabulario), len(vocabulario)), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota:** En vez de codificar los valores manualmente, se computa el vocabulario (alfabeto en este caso) a partir del texto, para que funcione en cualquier otro idioma con un alfabeto diferente.\n",
    "\n",
    "Se crea el codificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un codificador\n",
    "char_a_int = {char: i for i, char in enumerate(vocabulario)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculan las transiciones de bigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las transiciones de bigramas\n",
    "for nombre in nombres:\n",
    "    for i in range(len(nombre) - 1):\n",
    "        ix1 = char_a_int[nombre[i]]\n",
    "        ix2 = char_a_int[nombre[i + 1]]\n",
    "        tabla_busqueda[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualizan los conteos de bigramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar x, y y conteos para el diagrama de dispersión\n",
    "x       = [i for i in range(len(tabla_busqueda)) for _ in range(len(tabla_busqueda[0]))]\n",
    "y       = [j for _ in range(len(tabla_busqueda)) for j in range(len(tabla_busqueda[0]))]\n",
    "conteos = [tabla_busqueda[i][j] for i in range(len(tabla_busqueda)) for j in range(len(tabla_busqueda[0]))]\n",
    "\n",
    "# Visualizar los conteos de bigramas\n",
    "plt.figure(figsize = (10, 8))\n",
    "dispersion = plt.scatter(x, y, s = conteos, c = conteos, cmap = 'Blues', alpha = 0.7)\n",
    "plt.xticks(ticks = np.arange(len(vocabulario)), labels = vocabulario)\n",
    "plt.yticks(ticks = np.arange(len(vocabulario)), labels = vocabulario)\n",
    "plt.xlabel('Primer Carácter')\n",
    "plt.ylabel('Segundo Carácter')\n",
    "plt.title('Conteos de Bigrama')\n",
    "plt.colorbar(dispersion, label = 'Conteo')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota:** En este gráfico, las burbujas más grandes en la parte inferior izquierda, correspondientes a la primera mitad del alfabeto, indican que estos pares de caracteres ocurren con mayor frecuencia en el conjunto de datos. Lo anterior sugiere que los bigramas con letras del inicio del alfabeto son más comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Generar Nombres con el Modelo de Bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se crea la función `generar_nombre()` para generar nombres a partir de un modelo de lenguaje basado en bigramas. La función utiliza un enfoque probabilístico para seleccionar cada carácter en el nombre de acuerdo con la distribución de probabilidad que surge del carácter anterior en la tabla de búsqueda. El proceso continúa hasta encontrar el carácter de fin `>`, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se inicializa cada nombre con `<` y una `cadena_inicio` opcional.\n",
    "2. Se selecciona cada carácter sucesivo basándose en las probabilidades de bigramas hasta llegar al carácter de fin `>`.\n",
    "3. Se asegura que los nombres generados sean únicos y tengan al menos tres caracteres.\n",
    "4. Se generan e imprimen al menos 10 nombres únicos mediante `generar_nombre()`.\n",
    "\n",
    "Ejecutar el siguiente código para generar nombres con el Modelo de Bigrama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nombres con el Modelo de Lenguaje de Bigrama\n",
    "def generar_nombre(cadena_inicio = ''):\n",
    "    nombre = '<' + cadena_inicio.lower()\n",
    "    while True:\n",
    "        ix1 = char_a_int[nombre[-1]]\n",
    "        probabilidades_siguiente = tabla_busqueda[ix1]\n",
    "        peso_total = sum(probabilidades_siguiente)\n",
    "        if peso_total > 0:\n",
    "            siguiente_caracter = random.choices(vocabulario, weights = probabilidades_siguiente, k = 1)[0]\n",
    "        else:\n",
    "            siguiente_caracter = random.choice(vocabulario)\n",
    "        if siguiente_caracter == '>':\n",
    "            break\n",
    "        nombre += siguiente_caracter\n",
    "    return nombre[1:].capitalize()\n",
    "\n",
    "# Generar e imprimir 10 nombres únicos usando el Modelo de Lenguaje de Bigrama\n",
    "nombres_unicos = set()\n",
    "while len(nombres_unicos) < 10:\n",
    "    nombre = generar_nombre('Joe')    \n",
    "    if '<' + nombre.lower() + '>' not in nombres:\n",
    "        nombres_unicos.add(nombre)\n",
    "\n",
    "for nombre in nombres_unicos:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Se puede reemplazar `random.choices(vocabulario, weights = probabilidades_siguiente, k = 1)[0]` por `random.choices(vocabulario, k = 1)[0]` y comparar los nombres generados antes y después de este cambio para apreciar el efecto de diferentes esquemas de ponderación en el modelo de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Generar Nombres Utilizando Trigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se extiende el modelo de lenguaje previo para generar nombres utilizando trigramas, con el fin de lograr que el modelo capture patrones más complejos al considerar secuencias de tres caracteres en lugar de dos, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se crea una tabla de búsqueda llamada `tabla_busqueda_trigrama` con una tercera dimensión para almacenar los conteos de transiciones de trigramas.\n",
    "2. Se actualiza la función de generación previa; es decir, se debe crear `generar_nombre_trigrama()`, que selecciona el siguiente carácter de forma probabilística basándose en los dos caracteres previos.\n",
    "3. Se generan e imprimen al menos 10 nombres únicos utilizando `generar_nombre_trigrama()`, para observar patrones más realistas.\n",
    "\n",
    "Ejecutar el siguiente código para generar nombres con el Modelo de Trigrama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar las dimensiones de la tabla de búsqueda para trigramas\n",
    "tabla_busqueda_trigrama = torch.zeros((len(vocabulario), len(vocabulario), len(vocabulario)), dtype = torch.int32)\n",
    "\n",
    "# Calcular las transiciones de trigramas\n",
    "for nombre in nombres:\n",
    "    for i in range(len(nombre) - 2):\n",
    "        ix1 = char_a_int[nombre[i]]  \n",
    "        ix2 = char_a_int[nombre[i + 1]]\n",
    "        ix3 = char_a_int[nombre[i + 2]]\n",
    "        tabla_busqueda_trigrama[ix1, ix2, ix3] += 1\n",
    "        \n",
    "# Función del Modelo de Trigrama\n",
    "def generar_nombre_trigrama(cadena_inicio = ''):\n",
    "    nombre = '<' + cadena_inicio\n",
    "    while True:\n",
    "        if len(nombre) < 2:\n",
    "            siguiente_caracter = random.choice(vocabulario)\n",
    "        else:\n",
    "            ix1 = char_a_int[nombre[-2]]\n",
    "            ix2 = char_a_int[nombre[-1]]\n",
    "            probabilidades_siguiente = tabla_busqueda_trigrama[ix1, ix2]\n",
    "            peso_total = sum(probabilidades_siguiente)\n",
    "            if peso_total > 0:\n",
    "                siguiente_caracter = random.choices(vocabulario, weights = probabilidades_siguiente, k = 1)[0]\n",
    "            else:\n",
    "                siguiente_caracter = random.choice(vocabulario)\n",
    "        if siguiente_caracter == '>':\n",
    "            break\n",
    "        nombre += siguiente_caracter\n",
    "    return nombre[1:].capitalize()\n",
    "\n",
    "# Generar e imprimir 10 nombres únicos usando el Modelo de Lenguaje de Trigrama\n",
    "nombres_unicos = set()\n",
    "while len(nombres_unicos) < 10:\n",
    "    nombre = generar_nombre_trigrama('Joe')    \n",
    "    if '<' + nombre.lower() + '>' not in nombres:\n",
    "        nombres_unicos.add(nombre)\n",
    "\n",
    "for nombre in nombres_unicos:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase II. Mejorar los Modelos de Lenguaje con Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Definir un Decodificador y Convertir Caracteres a Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se definen los componentes esenciales para el procesamiento de datos necesario antes de entrenar el modelo de Red Neuronal Recurrente (RNN), realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. **Diccionario `int_a_char`**: Decodificador que mapea los índices enteros a caracteres del alfabeto.\n",
    "2. **Función `char_a_tensor(texto)`**: Convierte una cadena de caracteres en una representación tensorial. Las redes neuronales operan con datos numéricos, y los tensores son la estructura de datos base en PyTorch.\n",
    "\n",
    "Ejecutar el siguiente código para completar esta sección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificador\n",
    "int_a_char = {idx: char for idx, char in enumerate(vocabulario)}\n",
    "\n",
    "# Convertir una cadena de caracteres a un tensor de índices enteros\n",
    "def char_a_tensor(texto):\n",
    "    return torch.tensor([char_a_int[char] for char in texto], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Diseñar la Arquitectura de la RNN para la Modelización del Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se define un modelo de Red Neuronal Recurrente (RNN) personalizado con PyTorch. En consecuencia, se implementa la clase `RNNPersonalizada`, que hereda de `nn.Module` e incluye:\n",
    "\n",
    "1. Una capa de embedding\n",
    "2. Una capa GRU (Unidad Recurrente Gated)\n",
    "3. Una capa totalmente conectada (lineal) para la salida\n",
    "\n",
    "Las **GRU** abordan problemas de gradientes que desaparecen, lo que las hace efectivas para capturar dependencias en secuencias más largas al actualizar u 'olvidar' información de manera selectiva, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. El método `__init__`:\n",
    "- Define la capa de embedding para convertir los índices de entrada en vectores densos.\n",
    "- Define la capa **GRU** para procesar datos de secuencia con la dimensión oculta especificada.\n",
    "- Define la capa totalmente conectada para mapear la salida de la GRU al tamaño de salida deseado.\n",
    "2. En atención a lo cual, el método `__init__` toma los siguientes parámetros:\n",
    "- Tamaño de entrada (`tamaño_entrada`): El tamaño del vocabulario, que define el número de tokens únicos que pueden ser embebidos.\n",
    "- Tamaño de embedding (`tamaño_embed`): La dimensionalidad de la capa de embedding, que convierte cada token de entrada en una representación vectorial densa de este tamaño.\n",
    "- Dimensión oculta (`dim_oculta`): El número de características en el estado oculto de la **GRU**, lo que determina la capacidad de la RNN para capturar dependencias de secuencias.\n",
    "- Tamaño de la salida (`tamaño_salida`): El número de clases de salida o dimensiones objetivo utilizadas por la capa totalmente conectada para generar la salida final.\n",
    "3. El método `forward()`:\n",
    "- Determina cómo se procesan los datos de entrada a través de las capas para generar predicciones de salida.\n",
    "4. En atención a lo cual, en el método `forward()` se:\n",
    "- Aplica la capa de embedding a la entrada $x$. Así pues, se reestructura para su procesamiento por la **GRU**. Por lo tanto, la entrada termina pasanda a través de la **GRU**.\n",
    "- Utiliza la capa totalmente conectada para generar la salida final\n",
    "\n",
    "Ejecutar el siguiente código para completar esta sección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Personalizada\n",
    "class RNNPersonalizada(nn.Module):\n",
    "    def __init__(self, tamaño_entrada, tamaño_embed, dim_oculta, tamaño_salida):\n",
    "        super(RNNPersonalizada, self).__init__()\n",
    "        self.embedding  = nn.Embedding(num_embeddings = tamaño_entrada, embedding_dim = tamaño_embed)\n",
    "        self.gru        = nn.GRU(input_size = tamaño_embed, hidden_size = dim_oculta)\n",
    "        self.fc         = nn.Linear(dim_oculta, tamaño_salida)\n",
    "        self.dim_oculta = dim_oculta\n",
    "\n",
    "    def forward(self, x, oculto):\n",
    "        x               = self.embedding(x).view(1, 1, -1)\n",
    "        salida, oculto  = self.gru(x, oculto)\n",
    "        salida          = self.fc(salida.view(1, -1))\n",
    "        return salida, oculto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3  Escribir Funciones para Generar el Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se implementan funciones para generar nombres usando un modelo RNN. La función comienza por inicializar el estado oculto con el contexto de la cadena de inicio proporcionada (`cadena_inicio`), preparando el modelo para generar texto que fluya naturalmente a partir de la secuencia inicial. Luego, genera secuencialmente caracteres pasando el último carácter y el estado oculto al modelo, actualizando el estado oculto en cada paso, hasta que se genera el token de fin `>` o se alcanza la `longitud` especificada, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se define la función `generar_nombre_rnn()` para generar nombres usando un modelo RNN preentrenado. Dicha función recibe como parámetros de entrada o argumentos el `modelo`, la `cadena_inicio`, la `longitud` y la `temperatura`. Finalmente, genera texto de la siguiente manera:\n",
    "\n",
    "- Se inicializa el nombre con `cadena_inicio`, precedido por un token de inicio `<`, y lo convierte en un tensor usando la función `char_a_tensor()` definida previamente.  \n",
    "- Se crea un estado oculto inicial para la RNN usando `torch.zeros` con dimensiones que coincidan con el tamaño oculto del modelo.  \n",
    "- Se itera a través de `cadena_inicio` para inicializar el estado oculto, pasando cada tensor de carácter al modelo y estado oculto.  \n",
    "- Se comienza a generar caracteres de uno en uno hasta alcanzar la `longitud` especificada:  \n",
    "    a) Se usa el modelo para predecir el siguiente carácter en función del tensor de carácter actual y el estado oculto.  \n",
    "    b) Se aplica escalado de temperatura para controlar la aleatoriedad en la selección de caracteres. A continuación, selecciona el siguiente carácter de forma probabilística a partir de la distribución de salida.  \n",
    "    c) Se anexa el carácter predicho al texto generado, deteniéndote si se alcanza el token de fin `>`.  \n",
    "- Se devuelve el nombre final generado como una cadena en mayúscula inicial y sin el token de inicio.\n",
    "\n",
    "2. Se define la función `generar_nombres_unicos()` para generar e imprimir un conjunto de nombres únicos, usando `generar_nombre_rnn()` con el objetivo de crear cada uno. Dicha función recibe como parámetros de entrada o argumentos el `model`, `cadena_inicio` y  `n` como la cantidad de nombres que se desean generar. Finalmente, procede de la siguiente manera:\n",
    "\n",
    "- Se inicializa un conjunto vacío para almacenar los nombres únicos.  \n",
    "- En un bucle, se llama a la función `generar_nombre_rnn()` para generar un nuevo nombre usando la cadena de inicio proporcionada y longitud máxima `MAX_LONGITUD_NOMBRE`.  \n",
    "- Agrega cada nombre generado al conjunto. El uso de un conjunto garantiza que no se dupliquen nombres.  \n",
    "- Se continúa generando nombres hasta que se hayan reunido `n` nombres únicos.  \n",
    "- Se imprime cada nombre único generado por el modelo.\n",
    "\n",
    "Ejecutar el siguiente código para completar esta sección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LONGITUD_NOMBRE         = 32\n",
    "\n",
    "# Generar texto con el modelo\n",
    "def generar_nombre_rnn(modelo, cadena_inicio = '', longitud = MAX_LONGITUD_NOMBRE, temperatura = 0.8):\n",
    "    cadena_inicio           = '<' + cadena_inicio.lower()\n",
    "    tensor_entrada          = char_a_tensor(cadena_inicio)\n",
    "    oculto                  = torch.zeros(1, 1, modelo.dim_oculta)\n",
    "    \n",
    "    for i in range(len(cadena_inicio) - 1):\n",
    "        _, oculto           = modelo(tensor_entrada[i].unsqueeze(0), oculto)\n",
    "    texto_generado          = cadena_inicio[1:]\n",
    "    caracter_entrada        = tensor_entrada[-1].unsqueeze(0)\n",
    "    \n",
    "    for _ in range(longitud):\n",
    "        salida, oculto      = modelo(caracter_entrada.unsqueeze(0), oculto)\n",
    "        distribucion_salida = salida.data.view(-1).div(temperatura).exp()\n",
    "        indice_top          = torch.multinomial(distribucion_salida, 1)[0]\n",
    "        siguiente_caracter  = int_a_char[indice_top.item()]\n",
    "        if siguiente_caracter == '>':\n",
    "            break\n",
    "        texto_generado      += siguiente_caracter\n",
    "        caracter_entrada    = char_a_tensor(siguiente_caracter)\n",
    "    \n",
    "    return texto_generado.capitalize()\n",
    "\n",
    "# Generar e imprimir n nombres únicos usando el Modelo de Lenguaje RNN\n",
    "def generar_nombres_unicos(modelo, cadena_inicio, n):\n",
    "    nombres_unicos = set()\n",
    "    while len(nombres_unicos) < n:\n",
    "        nombre = generar_nombre_rnn(modelo, cadena_inicio, MAX_LONGITUD_NOMBRE)\n",
    "        if '<' + nombre.lower() + '>' not in nombres:\n",
    "            nombres_unicos.add(nombre)\n",
    "    \n",
    "    for nombre in nombres_unicos:\n",
    "        print(nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Entrenar el Modelo RNN Personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "En esta sección, se entrena un modelo RNN personalizado usando el conjunto de datos proporcionado para aprender los patrones y la estructura subyacentes de los datos de texto, realizando las siguientes operaciones en esta sección:\n",
    "\n",
    "1. Se definen los parámetros de entrenamiento para el tamaño de embedding, la dimensión oculta y el número total de épocas.  \n",
    "2. Se inicializa el modelo, el optimizador y la función de pérdida para configurar el entrenamiento.  \n",
    "3. Se implementa un bucle de entrenamiento para iterar en cada época:  \n",
    "    - Se baraja el conjunto de datos de nombres al inicio de cada época para garantizar un orden de entrenamiento variado. Ahora bien, por cada nombre en el conjunto de datos:  \n",
    "    - Se convierte el nombre a tensores de entrada y objetivo usando `char_a_tensor()`.  \n",
    "    - Se inicializa el estado oculto para cada nombre nuevo.  \n",
    "    - Se reinician los gradientes acumulados en el optimizador.  \n",
    "    - Por cada carácter en el tensor de entrada:  \n",
    "        a) Se realiza un paso hacia adelante (forward pass) a través del modelo para obtener predicciones.  \n",
    "        b) Se calcula y acumula la pérdida comparando las predicciones con los objetivos.  \n",
    "    - Se procesa el nombre completo, retropropaga la pérdida acumulada y actualizan los parámetros del modelo.  \n",
    "4. Se registra y almacena la pérdida promedio de cada época; esto es, al final de cada época el progreso se agrega a una lista e imprime.  \n",
    "5. Se usa `generar_nombres_unicos()` después de cada época para generar nombres de ejemplo, lo cual permite observar el progreso del modelo.  \n",
    "6. Al completar todas las épocas, se grafica la pérdida registrada para visualizar el progreso del entrenamiento.\n",
    "\n",
    "Ejecutar el siguiente código para completar esta sección:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir los parámetros de entrenamiento\n",
    "TAMAÑO_EMBED = 32\n",
    "DIM_OCULTA   = 32\n",
    "EPOCAS       = 3\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "modelo       = RNNPersonalizada(len(vocabulario), TAMAÑO_EMBED, DIM_OCULTA, MAX_LONGITUD_NOMBRE)\n",
    "optimizador  = optim.Adam(modelo.parameters(), lr=0.005)\n",
    "criterio     = nn.CrossEntropyLoss()\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "perdidas     = []\n",
    "\n",
    "for epoca in range(EPOCAS):\n",
    "    perdida_total = 0\n",
    "    random.shuffle(nombres)  # Mezclar el conjunto de datos para asegurar un orden diferente en cada época\n",
    "    \n",
    "    for nombre in nombres:\n",
    "        entradas  = char_a_tensor(nombre[:-1])\n",
    "        objetivos = char_a_tensor(nombre[1:])\n",
    "        oculto    = torch.zeros(1, 1, DIM_OCULTA)\n",
    "        optimizador.zero_grad()\n",
    "        perdida   = 0\n",
    "    \n",
    "        for i in range(len(entradas)):\n",
    "            salida, oculto = modelo(entradas[i].unsqueeze(0), oculto)\n",
    "            perdida        += criterio(salida, objetivos[i].unsqueeze(0))\n",
    "        \n",
    "        perdida.backward()\n",
    "        optimizador.step()\n",
    "        perdida_total += perdida.item() / len(entradas)\n",
    "    \n",
    "    perdidas.append(perdida_total / len(nombres))\n",
    "    print(f'Época {epoca+1}, Pérdida: {perdida_total:.4f}')\n",
    "    generar_nombres_unicos(modelo, 'Joe', 10)\n",
    "    print('=' * 50)\n",
    "\n",
    "# Graficar la curva de pérdida\n",
    "plt.plot(range(1, EPOCAS + 1), perdidas)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida de Entrenamiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Se comparan los nombres generados con los obtenidos a través de bigramas y trigramas. Además, se observa el desempeño con distintas cadenas de inicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar habilidades mediante proyectos prácticos como este es una excelente manera de familiarizarse con nuevas técnicas y tecnologías. En consecuencia, para continuar experimentando con lo desarrollado, intente crear y optimizar otros modelos y validarlos en diversos conjuntos de datos de prueba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
